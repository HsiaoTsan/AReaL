{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e4f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b7b51dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vllm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgc\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'vllm'"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "399a81f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model path\n",
    "model_path = \"/home/data/Qwen/Qwen2.5-1.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98128c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-01 06:25:27 [config.py:717] This model supports multiple tasks: {'generate', 'embed', 'score', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 08-01 06:25:28 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 08-01 06:25:29 [core.py:58] Initializing a V1 LLM engine (v0.8.5) with config: model='/home/bruceli/projects/data/Qwen/Qwen3-1.7B', speculative_config=None, tokenizer='/home/bruceli/projects/data/Qwen/Qwen3-1.7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/bruceli/projects/data/Qwen/Qwen3-1.7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-01 06:25:30,894 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-01 06:25:31 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7efe82181070>\n",
      "INFO 08-01 06:25:31 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 08-01 06:25:31 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 08-01 06:25:31 [topk_topp_sampler.py:44] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.\n",
      "INFO 08-01 06:25:31 [gpu_model_runner.py:1329] Starting to load model /home/bruceli/projects/data/Qwen/Qwen3-1.7B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.96it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.95it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-01 06:25:32 [loader.py:458] Loading weights took 0.58 seconds\n",
      "INFO 08-01 06:25:32 [gpu_model_runner.py:1347] Model loading took 3.2152 GiB and 0.714961 seconds\n",
      "INFO 08-01 06:25:39 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/bee35a0a39/rank_0_0 for vLLM's torch.compile\n",
      "INFO 08-01 06:25:39 [backends.py:430] Dynamo bytecode transform time: 6.87 s\n",
      "INFO 08-01 06:25:43 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 4.000 s\n",
      "INFO 08-01 06:25:45 [monitor.py:33] torch.compile takes 6.87 s in total\n",
      "INFO 08-01 06:25:45 [kv_cache_utils.py:634] GPU KV cache size: 579,040 tokens\n",
      "INFO 08-01 06:25:45 [kv_cache_utils.py:637] Maximum concurrency for 40,960 tokens per request: 14.14x\n",
      "INFO 08-01 06:26:03 [gpu_model_runner.py:1686] Graph capturing finished in 18 secs, took 0.47 GiB\n",
      "INFO 08-01 06:26:03 [core.py:159] init engine (profile, create kv cache, warmup model) took 31.36 seconds\n",
      "INFO 08-01 06:26:03 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "# Create the LLM instance\n",
    "llm = LLM(model=model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12d914de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SamplingParams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Sampling parameters\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sampling_params = \u001b[43mSamplingParams\u001b[49m(\n\u001b[32m      3\u001b[39m     n=\u001b[32m1\u001b[39m,\n\u001b[32m      4\u001b[39m     temperature=\u001b[32m0.0\u001b[39m,  \u001b[38;5;66;03m# temperature=0.0 means Deterministic\u001b[39;00m\n\u001b[32m      5\u001b[39m     top_p=\u001b[32m1.0\u001b[39m,\n\u001b[32m      6\u001b[39m     max_tokens=\u001b[32m50\u001b[39m,\n\u001b[32m      7\u001b[39m     logprobs=\u001b[32m0\u001b[39m,  \u001b[38;5;66;03m# Request logprobs for each token\u001b[39;00m\n\u001b[32m      8\u001b[39m     prompt_logprobs=\u001b[32m0\u001b[39m,\n\u001b[32m      9\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'SamplingParams' is not defined"
     ]
    }
   ],
   "source": [
    "# Sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    n=1,\n",
    "    temperature=0.0,  # temperature=0.0 means Deterministic\n",
    "    top_p=1.0,\n",
    "    max_tokens=50,\n",
    "    logprobs=0,  # Request logprobs for each token\n",
    "    prompt_logprobs=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24b41007",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"What is the capital of France?\"\n",
    "# prompt2 = \"What is the average size of a domestic cat?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32e4de9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s, est. speed input: 22.91 toks/s, output: 163.60 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# Run generation\n",
    "outputs = llm.generate([prompt1], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c4d5dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the capital of France?\n",
      "Generated Output:\n",
      "{7281: Logprob(logprob=-1.2939517498016357, rank=1, decoded_token='ĠAlso')}\n",
      "{11: Logprob(logprob=-7.867782187531702e-06, rank=1, decoded_token=',')}\n",
      "{646: Logprob(logprob=-1.0789486169815063, rank=2, decoded_token='Ġcan')}\n",
      "{498: Logprob(logprob=-0.000494715350214392, rank=1, decoded_token='Ġyou')}\n",
      "{3291: Logprob(logprob=-0.06823074072599411, rank=1, decoded_token='Ġtell')}\n",
      "{752: Logprob(logprob=-0.00011002412065863609, rank=1, decoded_token='Ġme')}\n",
      "{911: Logprob(logprob=-0.11912998557090759, rank=1, decoded_token='Ġabout')}\n",
      "{279: Logprob(logprob=-0.06143637374043465, rank=1, decoded_token='Ġthe')}\n",
      "{11245: Logprob(logprob=-2.6074867248535156, rank=1, decoded_token='Ġfamous')}\n",
      "{8585: Logprob(logprob=-1.4622935056686401, rank=1, decoded_token='ĠFrench')}\n",
      "{29481: Logprob(logprob=-1.6184898614883423, rank=1, decoded_token='Ġpainter')}\n",
      "{879: Logprob(logprob=-0.2603050172328949, rank=1, decoded_token='Ġwho')}\n",
      "{23983: Logprob(logprob=-0.3911629617214203, rank=1, decoded_token='Ġpainted')}\n",
      "{279: Logprob(logprob=-0.49866563081741333, rank=1, decoded_token='Ġthe')}\n",
      "{98783: Logprob(logprob=-0.6234254837036133, rank=1, decoded_token='ĠMona')}\n",
      "{28556: Logprob(logprob=-0.0002747396647464484, rank=1, decoded_token='ĠLisa')}\n",
      "{30: Logprob(logprob=-0.2883538603782654, rank=1, decoded_token='?')}\n",
      "{3555: Logprob(logprob=-0.21202179789543152, rank=1, decoded_token='ĠWhat')}\n",
      "{374: Logprob(logprob=-0.05867813155055046, rank=1, decoded_token='Ġis')}\n",
      "{279: Logprob(logprob=-0.006424843333661556, rank=1, decoded_token='Ġthe')}\n",
      "{7772: Logprob(logprob=-1.421403408050537, rank=1, decoded_token='Ġlargest')}\n",
      "{11580: Logprob(logprob=-1.127267837524414, rank=1, decoded_token='Ġplanet')}\n",
      "{304: Logprob(logprob=-7.152555099310121e-07, rank=1, decoded_token='Ġin')}\n",
      "{1039: Logprob(logprob=-0.3132656514644623, rank=1, decoded_token='Ġour')}\n",
      "{12941: Logprob(logprob=-0.0019349202048033476, rank=1, decoded_token='Ġsolar')}\n",
      "{1849: Logprob(logprob=-4.768370445162873e-07, rank=1, decoded_token='Ġsystem')}\n",
      "{30: Logprob(logprob=-0.016854146495461464, rank=1, decoded_token='?')}\n",
      "{3555: Logprob(logprob=-0.5520616173744202, rank=1, decoded_token='ĠWhat')}\n",
      "{374: Logprob(logprob=-0.010599506087601185, rank=1, decoded_token='Ġis')}\n",
      "{279: Logprob(logprob=-0.005176118575036526, rank=1, decoded_token='Ġthe')}\n",
      "{11483: Logprob(logprob=-1.0928349494934082, rank=1, decoded_token='Ġchemical')}\n",
      "{7735: Logprob(logprob=-0.025399919599294662, rank=1, decoded_token='Ġsymbol')}\n",
      "{369: Logprob(logprob=-0.016086198389530182, rank=1, decoded_token='Ġfor')}\n",
      "{3015: Logprob(logprob=-0.8378194570541382, rank=1, decoded_token='Ġwater')}\n",
      "{30: Logprob(logprob=-0.0008288762182928622, rank=1, decoded_token='?')}\n",
      "{3555: Logprob(logprob=-0.5340191125869751, rank=1, decoded_token='ĠWhat')}\n",
      "{374: Logprob(logprob=-0.0031797345727682114, rank=1, decoded_token='Ġis')}\n",
      "{279: Logprob(logprob=-0.0026236893609166145, rank=1, decoded_token='Ġthe')}\n",
      "{6722: Logprob(logprob=-1.5562405586242676, rank=2, decoded_token='Ġcapital')}\n",
      "{315: Logprob(logprob=-0.048589713871479034, rank=1, decoded_token='Ġof')}\n",
      "{15344: Logprob(logprob=-1.086297869682312, rank=1, decoded_token='ĠItaly')}\n",
      "{30: Logprob(logprob=-0.0007731309160590172, rank=1, decoded_token='?')}\n",
      "{3555: Logprob(logprob=-0.14880569279193878, rank=1, decoded_token='ĠWhat')}\n",
      "{374: Logprob(logprob=-0.0024756519123911858, rank=1, decoded_token='Ġis')}\n",
      "{279: Logprob(logprob=-0.0011526852613314986, rank=1, decoded_token='Ġthe')}\n",
      "{7772: Logprob(logprob=-1.3988407850265503, rank=1, decoded_token='Ġlargest')}\n",
      "{3146: Logprob(logprob=-0.26279595494270325, rank=1, decoded_token='Ġcountry')}\n",
      "{304: Logprob(logprob=-0.002186885569244623, rank=1, decoded_token='Ġin')}\n",
      "{279: Logprob(logprob=-0.0030978568829596043, rank=1, decoded_token='Ġthe')}\n",
      "{1879: Logprob(logprob=-8.976056415122002e-05, rank=1, decoded_token='Ġworld')}\n"
     ]
    }
   ],
   "source": [
    "# Print tokens and log probabilities\n",
    "for output in outputs:\n",
    "    print(f\"Prompt: {output.prompt}\")\n",
    "    print(\"Generated Output:\")\n",
    "    # for token, logprob in zip(output.outputs[0].token_ids, output.outputs[0].logprobs):\n",
    "    for logprob in output.outputs[0].logprobs:\n",
    "        # decoded_token = llm.get_tokenizer().decode([token])\n",
    "        print(f\"{logprob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a022a74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs), len(outputs)  # len(outputs) == number of prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c8fc15f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=4, prompt='What is the capital of France?', prompt_token_ids=[3838, 374, 279, 6722, 315, 9625, 30], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=[None, {374: Logprob(logprob=-2.314427375793457, rank=2, decoded_token='Ġis')}, {279: Logprob(logprob=-0.1128106340765953, rank=1, decoded_token='Ġthe')}, {6722: Logprob(logprob=-8.558416366577148, rank=281, decoded_token='Ġcapital')}, {315: Logprob(logprob=-0.10791323333978653, rank=1, decoded_token='Ġof')}, {9625: Logprob(logprob=-3.009153127670288, rank=2, decoded_token='ĠFrance')}, {30: Logprob(logprob=-2.6586267948150635, rank=2, decoded_token='?')}], outputs=[CompletionOutput(index=0, text=' Also, can you tell me about the famous French painter who painted the Mona Lisa? What is the largest planet in our solar system? What is the chemical symbol for water? What is the capital of Italy? What is the largest country in the world', token_ids=[7281, 11, 646, 498, 3291, 752, 911, 279, 11245, 8585, 29481, 879, 23983, 279, 98783, 28556, 30, 3555, 374, 279, 7772, 11580, 304, 1039, 12941, 1849, 30, 3555, 374, 279, 11483, 7735, 369, 3015, 30, 3555, 374, 279, 6722, 315, 15344, 30, 3555, 374, 279, 7772, 3146, 304, 279, 1879], cumulative_logprob=-21.122595513572037, logprobs=[{7281: Logprob(logprob=-1.2939517498016357, rank=1, decoded_token='ĠAlso')}, {11: Logprob(logprob=-7.867782187531702e-06, rank=1, decoded_token=',')}, {646: Logprob(logprob=-1.0789486169815063, rank=2, decoded_token='Ġcan')}, {498: Logprob(logprob=-0.000494715350214392, rank=1, decoded_token='Ġyou')}, {3291: Logprob(logprob=-0.06823074072599411, rank=1, decoded_token='Ġtell')}, {752: Logprob(logprob=-0.00011002412065863609, rank=1, decoded_token='Ġme')}, {911: Logprob(logprob=-0.11912998557090759, rank=1, decoded_token='Ġabout')}, {279: Logprob(logprob=-0.06143637374043465, rank=1, decoded_token='Ġthe')}, {11245: Logprob(logprob=-2.6074867248535156, rank=1, decoded_token='Ġfamous')}, {8585: Logprob(logprob=-1.4622935056686401, rank=1, decoded_token='ĠFrench')}, {29481: Logprob(logprob=-1.6184898614883423, rank=1, decoded_token='Ġpainter')}, {879: Logprob(logprob=-0.2603050172328949, rank=1, decoded_token='Ġwho')}, {23983: Logprob(logprob=-0.3911629617214203, rank=1, decoded_token='Ġpainted')}, {279: Logprob(logprob=-0.49866563081741333, rank=1, decoded_token='Ġthe')}, {98783: Logprob(logprob=-0.6234254837036133, rank=1, decoded_token='ĠMona')}, {28556: Logprob(logprob=-0.0002747396647464484, rank=1, decoded_token='ĠLisa')}, {30: Logprob(logprob=-0.2883538603782654, rank=1, decoded_token='?')}, {3555: Logprob(logprob=-0.21202179789543152, rank=1, decoded_token='ĠWhat')}, {374: Logprob(logprob=-0.05867813155055046, rank=1, decoded_token='Ġis')}, {279: Logprob(logprob=-0.006424843333661556, rank=1, decoded_token='Ġthe')}, {7772: Logprob(logprob=-1.421403408050537, rank=1, decoded_token='Ġlargest')}, {11580: Logprob(logprob=-1.127267837524414, rank=1, decoded_token='Ġplanet')}, {304: Logprob(logprob=-7.152555099310121e-07, rank=1, decoded_token='Ġin')}, {1039: Logprob(logprob=-0.3132656514644623, rank=1, decoded_token='Ġour')}, {12941: Logprob(logprob=-0.0019349202048033476, rank=1, decoded_token='Ġsolar')}, {1849: Logprob(logprob=-4.768370445162873e-07, rank=1, decoded_token='Ġsystem')}, {30: Logprob(logprob=-0.016854146495461464, rank=1, decoded_token='?')}, {3555: Logprob(logprob=-0.5520616173744202, rank=1, decoded_token='ĠWhat')}, {374: Logprob(logprob=-0.010599506087601185, rank=1, decoded_token='Ġis')}, {279: Logprob(logprob=-0.005176118575036526, rank=1, decoded_token='Ġthe')}, {11483: Logprob(logprob=-1.0928349494934082, rank=1, decoded_token='Ġchemical')}, {7735: Logprob(logprob=-0.025399919599294662, rank=1, decoded_token='Ġsymbol')}, {369: Logprob(logprob=-0.016086198389530182, rank=1, decoded_token='Ġfor')}, {3015: Logprob(logprob=-0.8378194570541382, rank=1, decoded_token='Ġwater')}, {30: Logprob(logprob=-0.0008288762182928622, rank=1, decoded_token='?')}, {3555: Logprob(logprob=-0.5340191125869751, rank=1, decoded_token='ĠWhat')}, {374: Logprob(logprob=-0.0031797345727682114, rank=1, decoded_token='Ġis')}, {279: Logprob(logprob=-0.0026236893609166145, rank=1, decoded_token='Ġthe')}, {6722: Logprob(logprob=-1.5562405586242676, rank=2, decoded_token='Ġcapital')}, {315: Logprob(logprob=-0.048589713871479034, rank=1, decoded_token='Ġof')}, {15344: Logprob(logprob=-1.086297869682312, rank=1, decoded_token='ĠItaly')}, {30: Logprob(logprob=-0.0007731309160590172, rank=1, decoded_token='?')}, {3555: Logprob(logprob=-0.14880569279193878, rank=1, decoded_token='ĠWhat')}, {374: Logprob(logprob=-0.0024756519123911858, rank=1, decoded_token='Ġis')}, {279: Logprob(logprob=-0.0011526852613314986, rank=1, decoded_token='Ġthe')}, {7772: Logprob(logprob=-1.3988407850265503, rank=1, decoded_token='Ġlargest')}, {3146: Logprob(logprob=-0.26279595494270325, rank=1, decoded_token='Ġcountry')}, {304: Logprob(logprob=-0.002186885569244623, rank=1, decoded_token='Ġin')}, {279: Logprob(logprob=-0.0030978568829596043, rank=1, decoded_token='Ġthe')}, {1879: Logprob(logprob=-8.976056415122002e-05, rank=1, decoded_token='Ġworld')}], finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6761b4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['add',\n",
       " 'encoder_prompt',\n",
       " 'encoder_prompt_token_ids',\n",
       " 'finished',\n",
       " 'from_seq_group',\n",
       " 'lora_request',\n",
       " 'metrics',\n",
       " 'multi_modal_placeholders',\n",
       " 'num_cached_tokens',\n",
       " 'outputs',\n",
       " 'prompt',\n",
       " 'prompt_logprobs',\n",
       " 'prompt_token_ids',\n",
       " 'request_id']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(outputs[0]) if not m.startswith('__')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b9fd1ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " {374: Logprob(logprob=-2.314427375793457, rank=2, decoded_token='Ġis')},\n",
       " {279: Logprob(logprob=-0.1128106340765953, rank=1, decoded_token='Ġthe')},\n",
       " {6722: Logprob(logprob=-8.558416366577148, rank=281, decoded_token='Ġcapital')},\n",
       " {315: Logprob(logprob=-0.10791323333978653, rank=1, decoded_token='Ġof')},\n",
       " {9625: Logprob(logprob=-3.009153127670288, rank=2, decoded_token='ĠFrance')},\n",
       " {30: Logprob(logprob=-2.6586267948150635, rank=2, decoded_token='?')}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].prompt_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e5cfdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3838 What\n",
      "   374  is\n",
      "   279  the\n",
      "  6722  capital\n",
      "   315  of\n",
      "  9625  France\n",
      "    30 ?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for token_id in outputs[0].prompt_token_ids:\n",
    "    print(f'{token_id:6d} {llm.get_tokenizer().decode([token_id])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ae79c71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CompletionOutput(index=0, text=' Also, can you tell me about the famous French painter who painted the Mona Lisa? What is the largest planet in our solar system? What is the chemical symbol for water? What is the capital of Italy? What is the largest country in the world', token_ids=[7281, 11, 646, 498, 3291, 752, 911, 279, 11245, 8585, 29481, 879, 23983, 279, 98783, 28556, 30, 3555, 374, 279, 7772, 11580, 304, 1039, 12941, 1849, 30, 3555, 374, 279, 11483, 7735, 369, 3015, 30, 3555, 374, 279, 6722, 315, 15344, 30, 3555, 374, 279, 7772, 3146, 304, 279, 1879], cumulative_logprob=-21.122595513572037, logprobs=[{7281: Logprob(logprob=-1.2939517498016357, rank=1, decoded_token='ĠAlso')}, {11: Logprob(logprob=-7.867782187531702e-06, rank=1, decoded_token=',')}, {646: Logprob(logprob=-1.0789486169815063, rank=2, decoded_token='Ġcan')}, {498: Logprob(logprob=-0.000494715350214392, rank=1, decoded_token='Ġyou')}, {3291: Logprob(logprob=-0.06823074072599411, rank=1, decoded_token='Ġtell')}, {752: Logprob(logprob=-0.00011002412065863609, rank=1, decoded_token='Ġme')}, {911: Logprob(logprob=-0.11912998557090759, rank=1, decoded_token='Ġabout')}, {279: Logprob(logprob=-0.06143637374043465, rank=1, decoded_token='Ġthe')}, {11245: Logprob(logprob=-2.6074867248535156, rank=1, decoded_token='Ġfamous')}, {8585: Logprob(logprob=-1.4622935056686401, rank=1, decoded_token='ĠFrench')}, {29481: Logprob(logprob=-1.6184898614883423, rank=1, decoded_token='Ġpainter')}, {879: Logprob(logprob=-0.2603050172328949, rank=1, decoded_token='Ġwho')}, {23983: Logprob(logprob=-0.3911629617214203, rank=1, decoded_token='Ġpainted')}, {279: Logprob(logprob=-0.49866563081741333, rank=1, decoded_token='Ġthe')}, {98783: Logprob(logprob=-0.6234254837036133, rank=1, decoded_token='ĠMona')}, {28556: Logprob(logprob=-0.0002747396647464484, rank=1, decoded_token='ĠLisa')}, {30: Logprob(logprob=-0.2883538603782654, rank=1, decoded_token='?')}, {3555: Logprob(logprob=-0.21202179789543152, rank=1, decoded_token='ĠWhat')}, {374: Logprob(logprob=-0.05867813155055046, rank=1, decoded_token='Ġis')}, {279: Logprob(logprob=-0.006424843333661556, rank=1, decoded_token='Ġthe')}, {7772: Logprob(logprob=-1.421403408050537, rank=1, decoded_token='Ġlargest')}, {11580: Logprob(logprob=-1.127267837524414, rank=1, decoded_token='Ġplanet')}, {304: Logprob(logprob=-7.152555099310121e-07, rank=1, decoded_token='Ġin')}, {1039: Logprob(logprob=-0.3132656514644623, rank=1, decoded_token='Ġour')}, {12941: Logprob(logprob=-0.0019349202048033476, rank=1, decoded_token='Ġsolar')}, {1849: Logprob(logprob=-4.768370445162873e-07, rank=1, decoded_token='Ġsystem')}, {30: Logprob(logprob=-0.016854146495461464, rank=1, decoded_token='?')}, {3555: Logprob(logprob=-0.5520616173744202, rank=1, decoded_token='ĠWhat')}, {374: Logprob(logprob=-0.010599506087601185, rank=1, decoded_token='Ġis')}, {279: Logprob(logprob=-0.005176118575036526, rank=1, decoded_token='Ġthe')}, {11483: Logprob(logprob=-1.0928349494934082, rank=1, decoded_token='Ġchemical')}, {7735: Logprob(logprob=-0.025399919599294662, rank=1, decoded_token='Ġsymbol')}, {369: Logprob(logprob=-0.016086198389530182, rank=1, decoded_token='Ġfor')}, {3015: Logprob(logprob=-0.8378194570541382, rank=1, decoded_token='Ġwater')}, {30: Logprob(logprob=-0.0008288762182928622, rank=1, decoded_token='?')}, {3555: Logprob(logprob=-0.5340191125869751, rank=1, decoded_token='ĠWhat')}, {374: Logprob(logprob=-0.0031797345727682114, rank=1, decoded_token='Ġis')}, {279: Logprob(logprob=-0.0026236893609166145, rank=1, decoded_token='Ġthe')}, {6722: Logprob(logprob=-1.5562405586242676, rank=2, decoded_token='Ġcapital')}, {315: Logprob(logprob=-0.048589713871479034, rank=1, decoded_token='Ġof')}, {15344: Logprob(logprob=-1.086297869682312, rank=1, decoded_token='ĠItaly')}, {30: Logprob(logprob=-0.0007731309160590172, rank=1, decoded_token='?')}, {3555: Logprob(logprob=-0.14880569279193878, rank=1, decoded_token='ĠWhat')}, {374: Logprob(logprob=-0.0024756519123911858, rank=1, decoded_token='Ġis')}, {279: Logprob(logprob=-0.0011526852613314986, rank=1, decoded_token='Ġthe')}, {7772: Logprob(logprob=-1.3988407850265503, rank=1, decoded_token='Ġlargest')}, {3146: Logprob(logprob=-0.26279595494270325, rank=1, decoded_token='Ġcountry')}, {304: Logprob(logprob=-0.002186885569244623, rank=1, decoded_token='Ġin')}, {279: Logprob(logprob=-0.0030978568829596043, rank=1, decoded_token='Ġthe')}, {1879: Logprob(logprob=-8.976056415122002e-05, rank=1, decoded_token='Ġworld')}], finish_reason=length, stop_reason=None)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].outputs  # outputs[prompt_idx].outputs[response_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3fe4c11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cumulative_logprob',\n",
       " 'finish_reason',\n",
       " 'finished',\n",
       " 'index',\n",
       " 'logprobs',\n",
       " 'lora_request',\n",
       " 'stop_reason',\n",
       " 'text',\n",
       " 'token_ids']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(outputs[0].outputs[0]) if not m.startswith('__')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a12ec63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{7281: Logprob(logprob=-1.2939517498016357, rank=1, decoded_token='ĠAlso')},\n",
       "  {11: Logprob(logprob=-7.867782187531702e-06, rank=1, decoded_token=',')},\n",
       "  {646: Logprob(logprob=-1.0789486169815063, rank=2, decoded_token='Ġcan')},\n",
       "  {498: Logprob(logprob=-0.000494715350214392, rank=1, decoded_token='Ġyou')},\n",
       "  {3291: Logprob(logprob=-0.06823074072599411, rank=1, decoded_token='Ġtell')},\n",
       "  {752: Logprob(logprob=-0.00011002412065863609, rank=1, decoded_token='Ġme')},\n",
       "  {911: Logprob(logprob=-0.11912998557090759, rank=1, decoded_token='Ġabout')},\n",
       "  {279: Logprob(logprob=-0.06143637374043465, rank=1, decoded_token='Ġthe')},\n",
       "  {11245: Logprob(logprob=-2.6074867248535156, rank=1, decoded_token='Ġfamous')},\n",
       "  {8585: Logprob(logprob=-1.4622935056686401, rank=1, decoded_token='ĠFrench')},\n",
       "  {29481: Logprob(logprob=-1.6184898614883423, rank=1, decoded_token='Ġpainter')},\n",
       "  {879: Logprob(logprob=-0.2603050172328949, rank=1, decoded_token='Ġwho')},\n",
       "  {23983: Logprob(logprob=-0.3911629617214203, rank=1, decoded_token='Ġpainted')},\n",
       "  {279: Logprob(logprob=-0.49866563081741333, rank=1, decoded_token='Ġthe')},\n",
       "  {98783: Logprob(logprob=-0.6234254837036133, rank=1, decoded_token='ĠMona')},\n",
       "  {28556: Logprob(logprob=-0.0002747396647464484, rank=1, decoded_token='ĠLisa')},\n",
       "  {30: Logprob(logprob=-0.2883538603782654, rank=1, decoded_token='?')},\n",
       "  {3555: Logprob(logprob=-0.21202179789543152, rank=1, decoded_token='ĠWhat')},\n",
       "  {374: Logprob(logprob=-0.05867813155055046, rank=1, decoded_token='Ġis')},\n",
       "  {279: Logprob(logprob=-0.006424843333661556, rank=1, decoded_token='Ġthe')},\n",
       "  {7772: Logprob(logprob=-1.421403408050537, rank=1, decoded_token='Ġlargest')},\n",
       "  {11580: Logprob(logprob=-1.127267837524414, rank=1, decoded_token='Ġplanet')},\n",
       "  {304: Logprob(logprob=-7.152555099310121e-07, rank=1, decoded_token='Ġin')},\n",
       "  {1039: Logprob(logprob=-0.3132656514644623, rank=1, decoded_token='Ġour')},\n",
       "  {12941: Logprob(logprob=-0.0019349202048033476, rank=1, decoded_token='Ġsolar')},\n",
       "  {1849: Logprob(logprob=-4.768370445162873e-07, rank=1, decoded_token='Ġsystem')},\n",
       "  {30: Logprob(logprob=-0.016854146495461464, rank=1, decoded_token='?')},\n",
       "  {3555: Logprob(logprob=-0.5520616173744202, rank=1, decoded_token='ĠWhat')},\n",
       "  {374: Logprob(logprob=-0.010599506087601185, rank=1, decoded_token='Ġis')},\n",
       "  {279: Logprob(logprob=-0.005176118575036526, rank=1, decoded_token='Ġthe')},\n",
       "  {11483: Logprob(logprob=-1.0928349494934082, rank=1, decoded_token='Ġchemical')},\n",
       "  {7735: Logprob(logprob=-0.025399919599294662, rank=1, decoded_token='Ġsymbol')},\n",
       "  {369: Logprob(logprob=-0.016086198389530182, rank=1, decoded_token='Ġfor')},\n",
       "  {3015: Logprob(logprob=-0.8378194570541382, rank=1, decoded_token='Ġwater')},\n",
       "  {30: Logprob(logprob=-0.0008288762182928622, rank=1, decoded_token='?')},\n",
       "  {3555: Logprob(logprob=-0.5340191125869751, rank=1, decoded_token='ĠWhat')},\n",
       "  {374: Logprob(logprob=-0.0031797345727682114, rank=1, decoded_token='Ġis')},\n",
       "  {279: Logprob(logprob=-0.0026236893609166145, rank=1, decoded_token='Ġthe')},\n",
       "  {6722: Logprob(logprob=-1.5562405586242676, rank=2, decoded_token='Ġcapital')},\n",
       "  {315: Logprob(logprob=-0.048589713871479034, rank=1, decoded_token='Ġof')},\n",
       "  {15344: Logprob(logprob=-1.086297869682312, rank=1, decoded_token='ĠItaly')},\n",
       "  {30: Logprob(logprob=-0.0007731309160590172, rank=1, decoded_token='?')},\n",
       "  {3555: Logprob(logprob=-0.14880569279193878, rank=1, decoded_token='ĠWhat')},\n",
       "  {374: Logprob(logprob=-0.0024756519123911858, rank=1, decoded_token='Ġis')},\n",
       "  {279: Logprob(logprob=-0.0011526852613314986, rank=1, decoded_token='Ġthe')},\n",
       "  {7772: Logprob(logprob=-1.3988407850265503, rank=1, decoded_token='Ġlargest')},\n",
       "  {3146: Logprob(logprob=-0.26279595494270325, rank=1, decoded_token='Ġcountry')},\n",
       "  {304: Logprob(logprob=-0.002186885569244623, rank=1, decoded_token='Ġin')},\n",
       "  {279: Logprob(logprob=-0.0030978568829596043, rank=1, decoded_token='Ġthe')},\n",
       "  {1879: Logprob(logprob=-8.976056415122002e-05, rank=1, decoded_token='Ġworld')}],\n",
       " 50)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].outputs[0].logprobs, len(outputs[0].outputs[0].logprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3e98a09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.6184898614883423"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].outputs[0].logprobs[10][29481].logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "83df7094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7281,\n",
       " 11,\n",
       " 646,\n",
       " 498,\n",
       " 3291,\n",
       " 752,\n",
       " 911,\n",
       " 279,\n",
       " 11245,\n",
       " 8585,\n",
       " 29481,\n",
       " 879,\n",
       " 23983,\n",
       " 279,\n",
       " 98783,\n",
       " 28556,\n",
       " 30,\n",
       " 3555,\n",
       " 374,\n",
       " 279,\n",
       " 7772,\n",
       " 11580,\n",
       " 304,\n",
       " 1039,\n",
       " 12941,\n",
       " 1849,\n",
       " 30,\n",
       " 3555,\n",
       " 374,\n",
       " 279,\n",
       " 11483,\n",
       " 7735,\n",
       " 369,\n",
       " 3015,\n",
       " 30,\n",
       " 3555,\n",
       " 374,\n",
       " 279,\n",
       " 6722,\n",
       " 315,\n",
       " 15344,\n",
       " 30,\n",
       " 3555,\n",
       " 374,\n",
       " 279,\n",
       " 7772,\n",
       " 3146,\n",
       " 304,\n",
       " 279,\n",
       " 1879]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].outputs[0].token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "070f52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cum_logprob = 0\n",
    "for d in outputs[0].outputs[0].logprobs:\n",
    "    for k,v in d.items():\n",
    "        cum_logprob += v.logprob\n",
    "\n",
    "assert cum_logprob == outputs[0].outputs[0].cumulative_logprob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "894a09f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].outputs[0].finished()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "467a1150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEPRECATE_INIT_POSARGS',\n",
       " 'DEPRECATE_LEGACY',\n",
       " 'apply_model',\n",
       " 'beam_search',\n",
       " 'chat',\n",
       " 'classify',\n",
       " 'collective_rpc',\n",
       " 'default_sampling_params',\n",
       " 'deprecate_legacy_api',\n",
       " 'embed',\n",
       " 'encode',\n",
       " 'engine_class',\n",
       " 'generate',\n",
       " 'get_default_sampling_params',\n",
       " 'get_tokenizer',\n",
       " 'llm_engine',\n",
       " 'request_counter',\n",
       " 'reset_prefix_cache',\n",
       " 'score',\n",
       " 'set_tokenizer',\n",
       " 'sleep',\n",
       " 'start_profile',\n",
       " 'stop_profile',\n",
       " 'wake_up']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(llm) if not m.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0236c1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CachedQwen2TokenizerFast(name_or_path='/home/bruceli/projects/data/Qwen/Qwen3-1.7B', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e5a22fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1447"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del llm\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "421e0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_vllm = outputs.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c5bc7",
   "metadata": {},
   "source": [
    "# SGLang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c3c229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/segment_wsl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# launch the offline engine\n",
    "import sglang as sgl\n",
    "\n",
    "from sglang.srt.conversation import chat_templates\n",
    "from sglang.test.test_utils import is_in_ci\n",
    "from sglang.utils import async_stream_and_merge, stream_and_merge\n",
    "\n",
    "if is_in_ci():\n",
    "    import patch\n",
    "else:\n",
    "    import nest_asyncio\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "\n",
    "import msgspec  # convert vLLM style sampling params to dict for SGLang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a104c5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]\n",
      "\n",
      "Capturing batches (bs=1 avail_mem=7.24 GB): 100%|██████████| 23/23 [00:22<00:00,  1.02it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sglang.srt.entrypoints.engine.Engine at 0x7f3c5b72b2c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_sgl = sgl.Engine(model_path=model_path)\n",
    "llm_sgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26400f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 你的 model_path 前面已定义\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4af9aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def show_meta(ret, max_show=20):\n",
    "    # llm_sgl.generate 单条返回是 dict；批量则是 list[dict]\n",
    "    item = ret[0] if isinstance(ret, list) else ret\n",
    "    meta = item.get(\"meta_info\", {})\n",
    "    print(\"=== meta_info keys ===\")\n",
    "    print(list(meta.keys()))\n",
    "\n",
    "    print(\"\\n=== finish_reason ===\")\n",
    "    print(json.dumps(meta.get(\"finish_reason\", {}), indent=2))\n",
    "\n",
    "    # 输出 token 级 logprob（前 max_show 个）\n",
    "    otl = meta.get(\"output_token_logprobs\", [])\n",
    "    print(f\"\\n=== output_token_logprobs (前{max_show}条) ===\")\n",
    "    print(otl[:max_show])\n",
    "\n",
    "    # 如果你把 logprob_start_len=0（见下一步），通常也会有 input_token_logprobs\n",
    "    itl = meta.get(\"input_token_logprobs\", [])\n",
    "    print(f\"\\n=== input_token_logprobs (前{max_show}条) ===\")\n",
    "    print(itl[:max_show])\n",
    "\n",
    "    # 还原输出文本看一下\n",
    "    output_token_ids = [x[1] for x in otl]\n",
    "    print(\"\\n=== decoded output text ===\")\n",
    "    print(tokenizer.decode(output_token_ids, skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2977af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== meta_info keys ===\n",
      "['id', 'finish_reason', 'prompt_tokens', 'input_token_logprobs', 'output_token_logprobs', 'completion_tokens', 'cached_tokens', 'e2e_latency']\n",
      "\n",
      "=== finish_reason ===\n",
      "{\n",
      "  \"type\": \"length\",\n",
      "  \"length\": 64\n",
      "}\n",
      "\n",
      "=== output_token_logprobs (前20条) ===\n",
      "[(-3.0709030628204346, 81917, None), (-0.4223562479019165, 26753, None), (-7.707350254058838, 1380, None), (-1.1138958930969238, 432, None), (-2.1303491592407227, 646, None), (-0.024493133649230003, 387, None), (-0.5073946714401245, 1483, None), (-3.182177782058716, 304, None), (-4.240322113037109, 6236, None), (-0.6424704790115356, 61905, None), (-1.0359283685684204, 13, None), (-3.9328532218933105, 2585, None), (-5.238398551940918, 1657, None), (-8.262171745300293, 14056, None), (-5.827347755432129, 1052, None), (-0.1627182811498642, 525, None), (-0.24745145440101624, 304, None), (-6.8705034255981445, 422, None), (-8.06794548034668, 3767, None), (-13.662675857543945, 9555, None)]\n",
      "\n",
      "=== input_token_logprobs (前20条) ===\n",
      "[(None, 840, None), (-0.7277117967605591, 20772, None), (-7.584022521972656, 26753, None), (-1.3608006238937378, 1128, None), (-12.137045860290527, 47496, None), (-6.300778388977051, 49716, None), (-0.3113216459751129, 374, None), (-3.1226959228515625, 13, None)]\n",
      "\n",
      "=== decoded output text ===\n",
      " Explain briefly where it can be used in chatbots. How many acts there are in Dany established dialect of Lakota and why is his journey as the Fire Drill lodger? What are Bhutanese women's rights as they exist? South Korea released a government report stating that 178 people were executed in\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain briefly what RLHF is.\"\n",
    "\n",
    "# 推荐直接传 input_ids，避免模板差异\n",
    "input_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "\n",
    "ret = llm_sgl.generate(\n",
    "    input_ids=input_ids,\n",
    "    sampling_params={\n",
    "        \"temperature\": 1.0,      # 你要非贪心就设 1.0\n",
    "        \"top_p\": 1.0,\n",
    "        \"max_new_tokens\": 64,\n",
    "    },\n",
    "    return_logprob=True,\n",
    "    logprob_start_len=0,         # 重点：你的“start_len=0”\n",
    "    # 可选：想拿 top-k 备选的 logprobs 就加下面这行\n",
    "    # top_logprobs_num=5,\n",
    ")\n",
    "\n",
    "show_meta(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbcdcd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== meta_info ===\n",
      "{'id': '844e3ef4a4db410eadb2c093db0cdacd', 'finish_reason': {'type': 'length', 'length': 32}, 'prompt_tokens': 7, 'input_token_logprobs': [(None, 3838, None), (-0.2663426697254181, 374, None), (-0.12340015918016434, 279, None), (-7.434597015380859, 6722, None), (-0.27008968591690063, 315, None), (-3.692899227142334, 9625, None), (-1.6525920629501343, 30, None)], 'output_token_logprobs': [(-0.7162180542945862, 576, None), (-0.014417066238820553, 6722, None), (-0.017033949494361877, 315, None), (-0.0002579425636213273, 9625, None), (-0.0032939265947788954, 374, None), (-0.0007614411297254264, 12095, None), (-2.1135082244873047, 382, None), (-9.087616920471191, 8078, None), (-6.802966594696045, 1493, None), (-2.899170398712158, 1378, None), (-0.2547663748264313, 22870, None), (-1.2431786060333252, 1119, None), (-0.6087610721588135, 264, None), (-2.2072386741638184, 3405, None), (-1.5188319683074951, 13, None), (-2.4839038848876953, 220, None), (-0.02767791412770748, 16, None), (-0.10039563477039337, 13, None), (-0.4778308570384979, 3555, None), (-0.04603041335940361, 374, None), (-0.02492990903556347, 279, None), (-0.06827174127101898, 6722, None), (-0.0186755433678627, 315, None), (-0.03694530576467514, 9625, None), (-0.3105734884738922, 30, None), (-0.29234766960144043, 220, None), (-0.05290015786886215, 17, None), (-0.0008454319322481751, 13, None), (-1.5386600494384766, 2160, None), (-0.7365440726280212, 279, None), (-0.002223995979875326, 6722, None), (-0.0020186996553093195, 315, None)], 'completion_tokens': 32, 'cached_tokens': 0, 'e2e_latency': 2.0539143085479736}\n",
      "\n",
      "=== output_tokens ===\n",
      "[576, 6722, 315, 9625, 374, 12095, 382, 8078, 1493, 1378, 22870, 1119, 264, 3405, 13, 220, 16, 13, 3555, 374, 279, 6722, 315, 9625, 30, 220, 17, 13, 2160, 279, 6722, 315]\n",
      "\n",
      "=== output_logprobs ===\n",
      "[-0.7162180542945862, -0.014417066238820553, -0.017033949494361877, -0.0002579425636213273, -0.0032939265947788954, -0.0007614411297254264, -2.1135082244873047, -9.087616920471191, -6.802966594696045, -2.899170398712158, -0.2547663748264313, -1.2431786060333252, -0.6087610721588135, -2.2072386741638184, -1.5188319683074951, -2.4839038848876953, -0.02767791412770748, -0.10039563477039337, -0.4778308570384979, -0.04603041335940361, -0.02492990903556347, -0.06827174127101898, -0.0186755433678627, -0.03694530576467514, -0.3105734884738922, -0.29234766960144043, -0.05290015786886215, -0.0008454319322481751, -1.5386600494384766, -0.7365440726280212, -0.002223995979875326, -0.0020186996553093195]\n",
      "\n",
      "=== decoded text ===\n",
      " The capital of France is Paris.\n",
      "\n",
      "Make these two sentences into a question. 1. What is the capital of France? 2. Is the capital of\n"
     ]
    }
   ],
   "source": [
    "# 1) 准备 prompt\n",
    "prompt = \"What is the capital of France?\"\n",
    "input_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "\n",
    "# 2) 调用 llm_sgl.generate\n",
    "result = llm_sgl.generate(\n",
    "    input_ids=input_ids,\n",
    "    sampling_params={\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"max_new_tokens\": 32,\n",
    "    },\n",
    "    return_logprob=True,\n",
    "    logprob_start_len=0,   # 关键参数\n",
    ")\n",
    "\n",
    "# 3) 提取 meta_info 和你关心的字段\n",
    "meta_info = result[\"meta_info\"]\n",
    "\n",
    "output_tokens = [x[1] for x in meta_info[\"output_token_logprobs\"]]\n",
    "output_logprobs = [x[0] for x in meta_info[\"output_token_logprobs\"]]\n",
    "\n",
    "# 4) 打印出来\n",
    "print(\"=== meta_info ===\")\n",
    "print(meta_info)\n",
    "\n",
    "print(\"\\n=== output_tokens ===\")\n",
    "print(output_tokens)\n",
    "\n",
    "print(\"\\n=== output_logprobs ===\")\n",
    "print(output_logprobs)\n",
    "\n",
    "# 5) 如果要解码成文字看看效果：\n",
    "decoded_text = tokenizer.decode(output_tokens, skip_special_tokens=False)\n",
    "print(\"\\n=== decoded text ===\")\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76838c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 1 decoded ===\n",
      " The capital of France is Paris.\n",
      "\n",
      "Who was the Emperor of France during World War 1 and World War 2?  The Emperor of France during World War\n",
      "\n",
      "=== meta_info (Step 2) ===\n",
      "{'id': '9bfad6bf11d14bb48bf1ba8110d98815', 'finish_reason': {'type': 'length', 'length': 32}, 'prompt_tokens': 39, 'input_token_logprobs': [(None, 3838, None), (-0.2639544904232025, 374, None), (-0.1274380087852478, 279, None), (-7.43554162979126, 6722, None), (-0.3023774325847626, 315, None), (-3.718020439147949, 9625, None), (-1.646174669265747, 30, None), (-0.673613965511322, 576, None), (-0.017382539808750153, 6722, None), (-0.017033590003848076, 315, None), (-0.0002907091984525323, 9625, None), (-0.0032928551081568003, 374, None), (-0.0008861667010933161, 12095, None), (-2.2133519649505615, 382, None), (-5.094367027282715, 15191, None), (-1.653369665145874, 572, None), (-0.3175278902053833, 279, None), (-5.7954912185668945, 34314, None), (-0.19512706995010376, 315, None), (-0.6719577312469482, 9625, None), (-1.0889030694961548, 2337, None), (-1.7123857736587524, 4337, None), (-0.0009180859779007733, 5004, None), (-2.5197296142578125, 220, None), (-0.49349820613861084, 16, None), (-3.319289207458496, 323, None), (-1.2153327465057373, 4337, None), (-0.02170300856232643, 5004, None), (-0.04399745166301727, 220, None), (-0.0028596720658242702, 17, None), (-0.036701273173093796, 30, None), (-5.10277795791626, 220, None), (-1.3900443315505981, 576, None), (-0.3056286573410034, 34314, None), (-0.007927857339382172, 315, None), (-0.0006526962388306856, 9625, None), (-0.04844208061695099, 2337, None), (-0.019290227442979813, 4337, None), (-0.005093692801892757, 5004, None)], 'output_token_logprobs': [(-1.1495940685272217, 358, None), (-0.10147208720445633, 323, None), (-0.05503178760409355, 4337, None), (-0.04171089455485344, 5004, None), (-0.005818932317197323, 7946, None), (-0.030650852248072624, 572, None), (-2.752098321914673, 66854, None), (-1.6596662998199463, 73210, None), (-0.5119182467460632, 17513, None), (-4.162193775177002, 32312, None), (-1.1628623008728027, 11876, None), (-0.9081668257713318, 15115, None), (-1.5698047876358032, 14927, None), (-1.073973298072815, 69427, None), (-1.1653404235839844, 1967, None), (-1.37013840675354, 10304, None), (-1.3723405599594116, 11, None), (-2.7692179679870605, 279, None), (-5.7134690284729, 20861, None), (-4.459357261657715, 382, None), (-4.795190334320068, 6986, None), (-1.5536749362945557, 279, None), (-6.375262260437012, 6277, None), (-1.278609275817871, 315, None), (-8.892792701721191, 7513, None), (-0.4653506278991699, 5837, None), (-9.161649703979492, 7172, None), (-3.088907241821289, 11941, None), (-1.3275612592697144, 1283, None), (-3.6301236152648926, 60030, None), (-4.859381198883057, 304, None), (-1.976366639137268, 12313, None)], 'completion_tokens': 32, 'cached_tokens': 0, 'e2e_latency': 0.16212677955627441}\n",
      "\n",
      "=== output_tokens (Step 2) ===\n",
      "[358, 323, 4337, 5004, 7946, 572, 66854, 73210, 17513, 32312, 11876, 15115, 14927, 69427, 1967, 10304, 11, 279, 20861, 382, 6986, 279, 6277, 315, 7513, 5837, 7172, 11941, 1283, 60030, 304, 12313]\n",
      "\n",
      "=== output_logprobs (Step 2) ===\n",
      "[-1.1495940685272217, -0.10147208720445633, -0.05503178760409355, -0.04171089455485344, -0.005818932317197323, -0.030650852248072624, -2.752098321914673, -1.6596662998199463, -0.5119182467460632, -4.162193775177002, -1.1628623008728027, -0.9081668257713318, -1.5698047876358032, -1.073973298072815, -1.1653404235839844, -1.37013840675354, -1.3723405599594116, -2.7692179679870605, -5.7134690284729, -4.459357261657715, -4.795190334320068, -1.5536749362945557, -6.375262260437012, -1.278609275817871, -8.892792701721191, -0.4653506278991699, -9.161649703979492, -3.088907241821289, -1.3275612592697144, -3.6301236152648926, -4.859381198883057, -1.976366639137268]\n",
      "\n",
      "=== Step 2 decoded ===\n",
      " I and World War II was Philippe Henri Albert Victor Louis Joseph Charles Napoleon Le Grand, the Third.\n",
      "\n",
      "Did the military of European countries increased significantly after WWII in comparison\n"
     ]
    }
   ],
   "source": [
    "# === 第一次生成的结果已经有了 ===\n",
    "print(\"\\n=== Step 1 decoded ===\")\n",
    "print(decoded_text)\n",
    "\n",
    "# === 拼接输出到 input_ids 再生成一次 ===\n",
    "input_ids2 = input_ids + output_tokens  # 把上一次生成的结果拼回去\n",
    "\n",
    "result2 = llm_sgl.generate(\n",
    "    input_ids=input_ids2,\n",
    "    sampling_params={\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"max_new_tokens\": 32,\n",
    "    },\n",
    "    return_logprob=True,\n",
    "    logprob_start_len=0,\n",
    ")\n",
    "\n",
    "meta_info2 = result2[\"meta_info\"]\n",
    "\n",
    "output_tokens2 = [x[1] for x in meta_info2[\"output_token_logprobs\"]]\n",
    "output_logprobs2 = [x[0] for x in meta_info2[\"output_token_logprobs\"]]\n",
    "\n",
    "print(\"\\n=== meta_info (Step 2) ===\")\n",
    "print(meta_info2)\n",
    "\n",
    "print(\"\\n=== output_tokens (Step 2) ===\")\n",
    "print(output_tokens2)\n",
    "\n",
    "print(\"\\n=== output_logprobs (Step 2) ===\")\n",
    "print(output_logprobs2)\n",
    "\n",
    "decoded_text2 = tokenizer.decode(output_tokens2, skip_special_tokens=False)\n",
    "print(\"\\n=== Step 2 decoded ===\")\n",
    "print(decoded_text2)\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e06a9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a0c353e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cached_tokens': 0,\n",
      " 'completion_tokens': 32,\n",
      " 'e2e_latency': 0.16212677955627441,\n",
      " 'finish_reason': {'length': 32, 'type': 'length'},\n",
      " 'id': '9bfad6bf11d14bb48bf1ba8110d98815',\n",
      " 'input_token_logprobs': [(None, 3838, None),\n",
      "                          (-0.2639544904232025, 374, None),\n",
      "                          (-0.1274380087852478, 279, None),\n",
      "                          (-7.43554162979126, 6722, None),\n",
      "                          (-0.3023774325847626, 315, None),\n",
      "                          (-3.718020439147949, 9625, None),\n",
      "                          (-1.646174669265747, 30, None),\n",
      "                          (-0.673613965511322, 576, None),\n",
      "                          (-0.017382539808750153, 6722, None),\n",
      "                          (-0.017033590003848076, 315, None),\n",
      "                          (-0.0002907091984525323, 9625, None),\n",
      "                          (-0.0032928551081568003, 374, None),\n",
      "                          (-0.0008861667010933161, 12095, None),\n",
      "                          (-2.2133519649505615, 382, None),\n",
      "                          (-5.094367027282715, 15191, None),\n",
      "                          (-1.653369665145874, 572, None),\n",
      "                          (-0.3175278902053833, 279, None),\n",
      "                          (-5.7954912185668945, 34314, None),\n",
      "                          (-0.19512706995010376, 315, None),\n",
      "                          (-0.6719577312469482, 9625, None),\n",
      "                          (-1.0889030694961548, 2337, None),\n",
      "                          (-1.7123857736587524, 4337, None),\n",
      "                          (-0.0009180859779007733, 5004, None),\n",
      "                          (-2.5197296142578125, 220, None),\n",
      "                          (-0.49349820613861084, 16, None),\n",
      "                          (-3.319289207458496, 323, None),\n",
      "                          (-1.2153327465057373, 4337, None),\n",
      "                          (-0.02170300856232643, 5004, None),\n",
      "                          (-0.04399745166301727, 220, None),\n",
      "                          (-0.0028596720658242702, 17, None),\n",
      "                          (-0.036701273173093796, 30, None),\n",
      "                          (-5.10277795791626, 220, None),\n",
      "                          (-1.3900443315505981, 576, None),\n",
      "                          (-0.3056286573410034, 34314, None),\n",
      "                          (-0.007927857339382172, 315, None),\n",
      "                          (-0.0006526962388306856, 9625, None),\n",
      "                          (-0.04844208061695099, 2337, None),\n",
      "                          (-0.019290227442979813, 4337, None),\n",
      "                          (-0.005093692801892757, 5004, None)],\n",
      " 'output_token_logprobs': [(-1.1495940685272217, 358, None),\n",
      "                           (-0.10147208720445633, 323, None),\n",
      "                           (-0.05503178760409355, 4337, None),\n",
      "                           (-0.04171089455485344, 5004, None),\n",
      "                           (-0.005818932317197323, 7946, None),\n",
      "                           (-0.030650852248072624, 572, None),\n",
      "                           (-2.752098321914673, 66854, None),\n",
      "                           (-1.6596662998199463, 73210, None),\n",
      "                           (-0.5119182467460632, 17513, None),\n",
      "                           (-4.162193775177002, 32312, None),\n",
      "                           (-1.1628623008728027, 11876, None),\n",
      "                           (-0.9081668257713318, 15115, None),\n",
      "                           (-1.5698047876358032, 14927, None),\n",
      "                           (-1.073973298072815, 69427, None),\n",
      "                           (-1.1653404235839844, 1967, None),\n",
      "                           (-1.37013840675354, 10304, None),\n",
      "                           (-1.3723405599594116, 11, None),\n",
      "                           (-2.7692179679870605, 279, None),\n",
      "                           (-5.7134690284729, 20861, None),\n",
      "                           (-4.459357261657715, 382, None),\n",
      "                           (-4.795190334320068, 6986, None),\n",
      "                           (-1.5536749362945557, 279, None),\n",
      "                           (-6.375262260437012, 6277, None),\n",
      "                           (-1.278609275817871, 315, None),\n",
      "                           (-8.892792701721191, 7513, None),\n",
      "                           (-0.4653506278991699, 5837, None),\n",
      "                           (-9.161649703979492, 7172, None),\n",
      "                           (-3.088907241821289, 11941, None),\n",
      "                           (-1.3275612592697144, 1283, None),\n",
      "                           (-3.6301236152648926, 60030, None),\n",
      "                           (-4.859381198883057, 304, None),\n",
      "                           (-1.976366639137268, 12313, None)],\n",
      " 'prompt_tokens': 39}\n"
     ]
    }
   ],
   "source": [
    "pprint(meta_info2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5898d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decoded_input_text': 'What is the capital of France? The capital of France '\n",
      "                       'is Paris.\\n'\n",
      "                       '\\n'\n",
      "                       'Who was the Emperor of France during World War 1 and '\n",
      "                       'World War 2?  The Emperor of France during World War',\n",
      " 'input_logprobs': [None,\n",
      "                    -0.2639544904232025,\n",
      "                    -0.1274380087852478,\n",
      "                    -7.43554162979126,\n",
      "                    -0.3023774325847626,\n",
      "                    -3.718020439147949,\n",
      "                    -1.646174669265747,\n",
      "                    -0.673613965511322,\n",
      "                    -0.017382539808750153,\n",
      "                    -0.017033590003848076,\n",
      "                    -0.0002907091984525323,\n",
      "                    -0.0032928551081568003,\n",
      "                    -0.0008861667010933161,\n",
      "                    -2.2133519649505615,\n",
      "                    -5.094367027282715,\n",
      "                    -1.653369665145874,\n",
      "                    -0.3175278902053833,\n",
      "                    -5.7954912185668945,\n",
      "                    -0.19512706995010376,\n",
      "                    -0.6719577312469482,\n",
      "                    -1.0889030694961548,\n",
      "                    -1.7123857736587524,\n",
      "                    -0.0009180859779007733,\n",
      "                    -2.5197296142578125,\n",
      "                    -0.49349820613861084,\n",
      "                    -3.319289207458496,\n",
      "                    -1.2153327465057373,\n",
      "                    -0.02170300856232643,\n",
      "                    -0.04399745166301727,\n",
      "                    -0.0028596720658242702,\n",
      "                    -0.036701273173093796,\n",
      "                    -5.10277795791626,\n",
      "                    -1.3900443315505981,\n",
      "                    -0.3056286573410034,\n",
      "                    -0.007927857339382172,\n",
      "                    -0.0006526962388306856,\n",
      "                    -0.04844208061695099,\n",
      "                    -0.019290227442979813,\n",
      "                    -0.005093692801892757],\n",
      " 'input_tokens': [3838,\n",
      "                  374,\n",
      "                  279,\n",
      "                  6722,\n",
      "                  315,\n",
      "                  9625,\n",
      "                  30,\n",
      "                  576,\n",
      "                  6722,\n",
      "                  315,\n",
      "                  9625,\n",
      "                  374,\n",
      "                  12095,\n",
      "                  382,\n",
      "                  15191,\n",
      "                  572,\n",
      "                  279,\n",
      "                  34314,\n",
      "                  315,\n",
      "                  9625,\n",
      "                  2337,\n",
      "                  4337,\n",
      "                  5004,\n",
      "                  220,\n",
      "                  16,\n",
      "                  323,\n",
      "                  4337,\n",
      "                  5004,\n",
      "                  220,\n",
      "                  17,\n",
      "                  30,\n",
      "                  220,\n",
      "                  576,\n",
      "                  34314,\n",
      "                  315,\n",
      "                  9625,\n",
      "                  2337,\n",
      "                  4337,\n",
      "                  5004]}\n"
     ]
    }
   ],
   "source": [
    "input_tokens2 = [x[1] for x in meta_info2[\"input_token_logprobs\"]]\n",
    "\n",
    "input_logprobs2 = [x[0] for x in meta_info2[\"input_token_logprobs\"]]\n",
    "\n",
    "decoded_input_text2 = tokenizer.decode(input_tokens2, skip_special_tokens=False)\n",
    "\n",
    "input_start_len_0 = dict(\n",
    "    input_tokens = input_tokens2,\n",
    "    input_logprobs = input_logprobs2,\n",
    "    decoded_input_text = decoded_input_text2,\n",
    "    output_tokens = [x[1] for x in meta_info2[\"output_token_logprobs\"]]\n",
    "    output_logprobs = [x[0] for x in meta_info2[\"output_token_logprobs\"]],\n",
    "\n",
    ")\n",
    "\n",
    "pprint(input_start_len_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6eb70cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_start_len_at_prompt_end = llm_sgl.generate(\n",
    "    input_ids=input_ids2,\n",
    "    sampling_params={\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"max_new_tokens\": 32,\n",
    "    },\n",
    "    return_logprob=True,\n",
    "    logprob_start_len=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cac2952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_ids, temperature=1., start_len=0):\n",
    "\n",
    "    response = llm_sgl.generate(\n",
    "    input_ids=input_ids,\n",
    "    sampling_params={\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": 1.0,\n",
    "        \"max_new_tokens\": 32,\n",
    "    },\n",
    "    return_logprob=True,\n",
    "    logprob_start_len=start_len,\n",
    "    )   \n",
    "\n",
    "\n",
    "    meta_info = response[\"meta_info\"]\n",
    "\n",
    "    input_tokens = [x[1] for x in meta_info[\"input_token_logprobs\"]]\n",
    "\n",
    "    input_logprobs = [x[0] for x in meta_info[\"input_token_logprobs\"]]\n",
    "\n",
    "    decoded_input_text = tokenizer.decode(input_tokens, skip_special_tokens=False)\n",
    "\n",
    "    result = dict(\n",
    "        input_tokens = input_tokens,\n",
    "        input_logprobs = input_logprobs,\n",
    "        decoded_input_text = decoded_input_text,\n",
    "        output_tokens = [x[1] for x in meta_info[\"output_token_logprobs\"]],\n",
    "        output_logprobs = [x[0] for x in meta_info[\"output_token_logprobs\"]],\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201030c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nConclusion:\\nsame parameter run twice:\\n- same input logprob at the same token\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(2):\n",
    "    results.append(generate_response(input_ids, temperature=0, start_len=3))\n",
    "\n",
    "print(np.allclose(results[0]['output_logprobs'], results[1]['output_logprobs']))\n",
    "print(np.allclose(results[0]['input_logprobs'][1:], results[1]['input_logprobs'][1:]))\n",
    "\n",
    "'''\n",
    "Conclusion:\n",
    "same parameter run twice (no matter what temperature is):\n",
    "- same input logprob at the same token\n",
    "'''\n",
    "\n",
    "'''\n",
    "Conclusion:\n",
    "same parameter run twice (temperature=0):\n",
    "- same output logprob at the same token\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdf21c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " capital of France? What is the capital of France?\n",
      "[-0.038739826530218124, -0.006982094142585993, -1.2796893119812012, -0.06121796742081642, -0.16203784942626953] [-0.03871792182326317, -0.006087571382522583, -1.3280937671661377, -0.05748198926448822, -0.15814797580242157]\n",
      "[None, -0.30019545555114746, -3.701012134552002, -1.647252082824707] [None, -0.2663426697254181, -0.12340015918016434, -7.434597015380859, -0.27008968591690063, -3.692899227142334, -1.6525920629501343]\n"
     ]
    }
   ],
   "source": [
    "res1 = generate_response(input_ids, temperature=0, start_len=3)\n",
    "res2 = generate_response(input_ids, temperature=0, start_len=0)\n",
    "print(res1['decoded_input_text'], res2['decoded_input_text'])\n",
    "print(res1['output_logprobs'][-5:], res2['output_logprobs'][-5:])\n",
    "print(res1['input_logprobs'], res2['input_logprobs'])\n",
    "\n",
    "'''\n",
    "Conclusion:\n",
    "same input, same temperature (even at 0), different start length leads to:\n",
    "- different input logprob at the same token\n",
    "- different output logprob at the same token\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7db1b85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " capital of France?  capital of France?My name is Elizabeth.\n",
      "[-0.038739826530218124, -0.006982094142585993, -1.2796893119812012, -0.06121796742081642, -0.16203784942626953] [-0.556067705154419, -2.174212694168091, -0.519115686416626, -0.9894031286239624, -0.25920653343200684]\n"
     ]
    }
   ],
   "source": [
    "res1 = generate_response(input_ids, temperature=0, start_len=3)\n",
    "res2 = generate_response(input_ids + tokenizer.encode('My name is Elizabeth.'), temperature=0, start_len=3)\n",
    "print(res1['decoded_input_text'], res2['decoded_input_text'])\n",
    "print(res1['output_logprobs'][-5:], res2['output_logprobs'][-5:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89f7e2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, -0.30019545555114746, -3.701012134552002, -1.647252082824707],\n",
       " [None,\n",
       "  -0.3002954125404358,\n",
       "  -3.7182364463806152,\n",
       "  -1.6308228969573975,\n",
       "  -14.545310020446777,\n",
       "  -3.7796692848205566,\n",
       "  -0.03700103610754013,\n",
       "  -7.269960880279541,\n",
       "  -1.4145187139511108]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[res1['input_logprobs'], res2['input_logprobs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d073a957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6722, 315, 9625, 30]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(' capital of France?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "13280d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3838, 374, 279, 6722, 315, 9625, 30]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08078814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.00789888110011816,\n",
       "  -0.0006735440110787749,\n",
       "  -0.04346011206507683,\n",
       "  -0.01717608980834484,\n",
       "  -0.0045041777193546295],\n",
       " [-0.007927857339382172,\n",
       "  -0.0006526962388306856,\n",
       "  -0.04844208061695099,\n",
       "  -0.019290227442979813,\n",
       "  -0.005093692801892757])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_logprobs3[-5:], input_logprobs2[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74cdd95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_start_len_at_prompt_end_run1 = input_start_len_at_prompt_end.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb129be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': [3838,\n",
       "  374,\n",
       "  279,\n",
       "  6722,\n",
       "  315,\n",
       "  9625,\n",
       "  30,\n",
       "  576,\n",
       "  6722,\n",
       "  315,\n",
       "  9625,\n",
       "  374,\n",
       "  12095,\n",
       "  382,\n",
       "  15191,\n",
       "  572,\n",
       "  279,\n",
       "  34314,\n",
       "  315,\n",
       "  9625,\n",
       "  2337,\n",
       "  4337,\n",
       "  5004,\n",
       "  220,\n",
       "  16,\n",
       "  323,\n",
       "  4337,\n",
       "  5004,\n",
       "  220,\n",
       "  17,\n",
       "  30,\n",
       "  220,\n",
       "  576,\n",
       "  34314,\n",
       "  315,\n",
       "  9625,\n",
       "  2337,\n",
       "  4337,\n",
       "  5004],\n",
       " 'input_logprobs': [None,\n",
       "  -0.2639544904232025,\n",
       "  -0.1274380087852478,\n",
       "  -7.43554162979126,\n",
       "  -0.3023774325847626,\n",
       "  -3.718020439147949,\n",
       "  -1.646174669265747,\n",
       "  -0.673613965511322,\n",
       "  -0.017382539808750153,\n",
       "  -0.017033590003848076,\n",
       "  -0.0002907091984525323,\n",
       "  -0.0032928551081568003,\n",
       "  -0.0008861667010933161,\n",
       "  -2.2133519649505615,\n",
       "  -5.094367027282715,\n",
       "  -1.653369665145874,\n",
       "  -0.3175278902053833,\n",
       "  -5.7954912185668945,\n",
       "  -0.19512706995010376,\n",
       "  -0.6719577312469482,\n",
       "  -1.0889030694961548,\n",
       "  -1.7123857736587524,\n",
       "  -0.0009180859779007733,\n",
       "  -2.5197296142578125,\n",
       "  -0.49349820613861084,\n",
       "  -3.319289207458496,\n",
       "  -1.2153327465057373,\n",
       "  -0.02170300856232643,\n",
       "  -0.04399745166301727,\n",
       "  -0.0028596720658242702,\n",
       "  -0.036701273173093796,\n",
       "  -5.10277795791626,\n",
       "  -1.3900443315505981,\n",
       "  -0.3056286573410034,\n",
       "  -0.007927857339382172,\n",
       "  -0.0006526962388306856,\n",
       "  -0.04844208061695099,\n",
       "  -0.019290227442979813,\n",
       "  -0.005093692801892757],\n",
       " 'decoded_input_text': 'What is the capital of France? The capital of France is Paris.\\n\\nWho was the Emperor of France during World War 1 and World War 2?  The Emperor of France during World War'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_start_len_at_prompt_end_run1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa05c03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': [3838,\n",
       "  374,\n",
       "  279,\n",
       "  6722,\n",
       "  315,\n",
       "  9625,\n",
       "  30,\n",
       "  576,\n",
       "  6722,\n",
       "  315,\n",
       "  9625,\n",
       "  374,\n",
       "  12095,\n",
       "  382,\n",
       "  15191,\n",
       "  572,\n",
       "  279,\n",
       "  34314,\n",
       "  315,\n",
       "  9625,\n",
       "  2337,\n",
       "  4337,\n",
       "  5004,\n",
       "  220,\n",
       "  16,\n",
       "  323,\n",
       "  4337,\n",
       "  5004,\n",
       "  220,\n",
       "  17,\n",
       "  30,\n",
       "  220,\n",
       "  576,\n",
       "  34314,\n",
       "  315,\n",
       "  9625,\n",
       "  2337,\n",
       "  4337,\n",
       "  5004],\n",
       " 'input_logprobs': [None,\n",
       "  -0.2639544904232025,\n",
       "  -0.1274380087852478,\n",
       "  -7.43554162979126,\n",
       "  -0.3023774325847626,\n",
       "  -3.718020439147949,\n",
       "  -1.646174669265747,\n",
       "  -0.673613965511322,\n",
       "  -0.017382539808750153,\n",
       "  -0.017033590003848076,\n",
       "  -0.0002907091984525323,\n",
       "  -0.0032928551081568003,\n",
       "  -0.0008861667010933161,\n",
       "  -2.2133519649505615,\n",
       "  -5.094367027282715,\n",
       "  -1.653369665145874,\n",
       "  -0.3175278902053833,\n",
       "  -5.7954912185668945,\n",
       "  -0.19512706995010376,\n",
       "  -0.6719577312469482,\n",
       "  -1.0889030694961548,\n",
       "  -1.7123857736587524,\n",
       "  -0.0009180859779007733,\n",
       "  -2.5197296142578125,\n",
       "  -0.49349820613861084,\n",
       "  -3.319289207458496,\n",
       "  -1.2153327465057373,\n",
       "  -0.02170300856232643,\n",
       "  -0.04399745166301727,\n",
       "  -0.0028596720658242702,\n",
       "  -0.036701273173093796,\n",
       "  -5.10277795791626,\n",
       "  -1.3900443315505981,\n",
       "  -0.3056286573410034,\n",
       "  -0.007927857339382172,\n",
       "  -0.0006526962388306856,\n",
       "  -0.04844208061695099,\n",
       "  -0.019290227442979813,\n",
       "  -0.005093692801892757],\n",
       " 'decoded_input_text': 'What is the capital of France? The capital of France is Paris.\\n\\nWho was the Emperor of France during World War 1 and World War 2?  The Emperor of France during World War'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_start_len_at_prompt_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "99c86de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_start_len_at_prompt_end == input_start_len_at_prompt_end_run1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7edc9f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 39)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_start_len_at_prompt_end['input_logprobs']), len(input_start_len_at_prompt_end_run1['input_logprobs']), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "43676a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.007927857339382172,\n",
       "  -0.0006526962388306856,\n",
       "  -0.04844208061695099,\n",
       "  -0.019290227442979813,\n",
       "  -0.005093692801892757],\n",
       " [-0.007927857339382172,\n",
       "  -0.0006526962388306856,\n",
       "  -0.04844208061695099,\n",
       "  -0.019290227442979813,\n",
       "  -0.005093692801892757])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_start_len_at_prompt_end['input_logprobs'][-5:], input_start_len_at_prompt_end_run1['input_logprobs'][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "18b3bc0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'output_logprobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43minput_start_len_at_prompt_end\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moutput_logprobs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[-\u001b[32m5\u001b[39m:], input_start_len_at_prompt_end_run1[\u001b[33m'\u001b[39m\u001b[33moutput_logprobs\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m5\u001b[39m:]\n",
      "\u001b[31mKeyError\u001b[39m: 'output_logprobs'"
     ]
    }
   ],
   "source": [
    "input_start_len_at_prompt_end['output_logprobs'][-5:], input_start_len_at_prompt_end_run1['output_logprobs'][-5:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (segment_wsl)",
   "language": "python",
   "name": "segment_wsl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
